lorenz:
    K: 8
    J: 32
    h: 1
    b: 10.0
    c: 10.0
    F: 20.0
    time_step: 0.001
    num_steps: 2000000
    skip: 5
    burn_in: 2000
    train_test_split: 0.5
gan:
    structure: conv
    t_skip: 5
    x_skip: 1
    output: sample
    cond_inputs: ["X_t", "U_t"]
    generator:
        num_cond_inputs: 2
        num_random_inputs: 4
        num_outputs: 32
        activation: leaky
        min_conv_filters: 8
        min_data_width: 4
        filter_width: 5
        dropout_alpha: 0.4
        normalize: 1
    discriminator:
        num_cond_inputs: 2
        num_sample_inputs: 32
        activation: leaky
        min_conv_filters: 8
        min_data_width: 4
        filter_width: 5
        dropout_alpha: 0.2
    gan_path: ./exp_20
    batch_size: 512
    gan_index: 20
    loss: binary_crossentropy
    learning_rate: 0.0002
    num_epochs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    metrics: ["accuracy"]
random_updater:
    out_file: /glade/p/work/dgagne/lorenz/exp_20/ar1_random_updater.pkl
histogram:
    num_x_bins: 30
    num_u_bins: 30
    out_file: /glade/p/work/dgagne/exp_20/u_histogram.pkl
poly:
    num_terms: 3
    noise_type: additive
    out_file: /glade/p/work/dgagne/exp_20/u_poly.pkl
poly_add:
    num_terms: 3
    out_file: /glade/p/work/dgagne/exp_20/u_poly_add.pkl
ann_res:
  mean_inputs: 1
  hidden_layers: 2
  hidden_neurons: 8
  activation: selu
  l2_weight: 0.01
  learning_rate: 0.001
  mean_loss: mse
  res_loss: kullback_leibler_divergence
  noise_sd: 1
  beta_1: 0.9
  model_path: /glade/p/work/dgagne/exp_20/
  dropout_alpha: 0.2
  num_epochs: 20
  batch_size: 1024
  val_split: 0.5
  verbose: 2
  model_config: 20
output_nc_file: /glade/p/work/dgagne/exp_20/lorenz_output.nc
output_csv_file: /glade/p/work/dgagne/exp_20/lorenz_combined_output.csv
