lorenz:
    K: 4
    J: 16
    h: 1.0
    b: 10.0
    c: 10.0
    F: 15.0
    time_step: 0.001
    num_steps: 1001000
    skip: 5
    burn_in: 1000
    train_test_split: 0.6
gan:
    structure: specified_random
    t_skip: 1
    x_skip: 1
    output: sample
    cond_inputs: ["X_t"]
    output_cols: ["Ux_t+1"]
    generator:
        num_cond_inputs: 1
        num_random_inputs: 1
        num_outputs: 1
        num_hidden_layers: 2
        num_hidden_neurons: 16
        activation: selu
        l2_strength: 0.0001
        use_dropout: 0
        noise_sd: 0.01
        min_exp: -1
        normalize: 1

    discriminator:
        num_cond_inputs: 1
        num_sample_inputs: 1
        activation: selu
        num_hidden_layers: 2
        num_hidden_neurons: 16
        l2_strength: 0.0001
        use_dropout: 0
        dropout_alpha: 0.0
        use_noise: 1
        noise_sd: 0.01
    gan_path: ./exp_pierre
    batch_size: 1024
    gan_index: 0
    loss: binary_crossentropy
    learning_rate: 0.0001
    num_epochs: [1, 10, 20, 30]
    metrics: ["accuracy"]
random_updater:
    out_file: ./exp_pierre/ar1_random_updater.pkl
histogram:
    num_x_bins: 30
    num_u_bins: 30
    out_file: ./exp_pierre/u_histogram.pkl
poly:
    num_terms: 3
    noise_type: additive
    out_file: ./exp_pierre/u_poly.pkl
poly_add:
    num_terms: 3
    out_file: ./exp_pierre/u_poly_add.pkl
ann:
  inputs: 1
  hidden_layers: 2
  hidden_neurons: 16
  activation: selu
  l2_weight: 0.0001
  learning_rate: 0.001
  loss: mse
  noise_sd: 0
  beta_1: 0.99
  out_path: "./exp_pierre"
  dropout_alpha: 0.1
  num_epochs: 30
  batch_size: 1024
  verbose: 1
  model_config: 5
output_nc_file: ./exp_pierre/lorenz_output.nc
output_csv_file: ./exp_pierre/lorenz_combined_output.csv
